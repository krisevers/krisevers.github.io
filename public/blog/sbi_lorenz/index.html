<!DOCTYPE html>
<html>
  <head id="head">
    <meta charset="utf-8">
    <meta name="viewport" content="initial-scale=1">
    <title>Kris Evers</title>
    <link type="text/css" rel="stylesheet" href="/style.css">
    <link type="text/css" rel="stylesheet" href="/highlight.css">
    <!-- mathtex -->
    <link type="text/css" rel="stylesheet" href="/Temml-Local.css">
    <script defer src="/temml.min.js"></script>
    <script defer src="/render-mathtex.js"></script>
    <!-- /mathtex -->
    
  <style>
    .date {
      margin-top: 15px;
      text-align: center;
    }
    .title h3{
      margin-top: 0;
    }


    figure {
        align-self: center;
        text-align: center;
    }
  </style>

  </head>
  <body id="body">

    <nav>
        <div class="nav-container">
            <a class="nav-brand" href="/">Kris Evers</a>
        </div>
        <div class="nav-container">
            <a class="nav-item" href="/">About</a>
            <a class="nav-item" href="/blog/">Posts</a>
            <!--<a class="nav-item" href="$site.page('projects').link()">Projects</a>-->
            <a class="nav-item" href="/publications/">Publications</a>
            <a class="nav-item nav-item-last" href="/bookmarks/">Bookmarks</a>
        </div>
    </nav>

    
  <span class="date">May 25, 2024</span>
  <h1>Simulation Based Inference on the Lorenz System</h1>
  <div><p>Central to many questions in neuroscience is how signal changes relate to underlying neural mechanisms during particular cognitive tasks. In general a hypothesis about the underlying mechanism is cast into a mechanistc or probabilistic model and some free parameters are optimized to fit the empirical data. Several issues come up with standard Bayesian inference methods: the full training procedure has to be performed for each source of data that one wants to infer, and the uncertainty and interactions between parameters remain unknown. For these reasons among others simulation based inference (SBI) methods are developed (sometimes called Generalized Bayesian Inference (GBI)). Here you have a similar unknown black-box mechanistic model (can be non-linear and highly complex), meaning the internal mechanisms are not accessible and do not have to be differentiable. The black-box model is only required to take a set of parameters (<script type="math/tex">\theta</script>) and generate a set of observations (<script type="math/tex">\psi</script>). In this way we can formulate this black-box mechanistic model as an unknown function (<script type="math/tex">F</script>):</p><script type="math/tex">\psi = F_{\theta}(x)
</script><p>Where <script type="math/tex">F</script> is conditioned on the free parameters (<script type="math/tex">\theta</script>, e.g. connectivity), and <script type="math/tex">x</script> are the experimental factors such as stimulus onset and duration (i.e. parameters which are not of interest for inversion). Taking a set of samples from a prior distribution <script type="math/tex">p(\theta)</script> of the parameters of interest one can run a number of simulations (<script type="math/tex">N</script>) and obtain a dataset containing the instantiations of the parameters and the resulting observations (<script type="math/tex">[\theta_i,\psi_i]_{i=1}^N</script>). This dataset is then a set of samples taken from:</p><script type="math/tex">p(\theta,\psi)=p(\theta)p(\psi,\theta)
</script><p>This dataset can be used to train a neural density estimator (specific kind of deep neural network for training distributions instead of classes) which will learn the distribution which quantifies the relationship between parameters and observations. We can apply new observation data to this distribution and infer an approximation of the posterior <script type="math/tex">p(\theta,\psi_{new})</script>. In other words, we can approximate the set of parameters given a new set of observations. This methods allows for amortized inference afeter the training procedure is complete (i.e. simulations of the model are only run to obtain the dataset, after training simulating is not required anymore).</p><p><figure><img src="/blog/sbi_lorenz/sbi_hh.png">
<figcaption><em>Figure 1: Example simulation based inference procedure on Hodgin-Huxley spiking neuron model.</em></figcaption></figure></p><h3>Inference of the Lorenz System</h3><p>To show the power of neural networks in learning distributions, and thus of underlying model parameters given some set of training data, I will show some results of the method applied to the Lorenz System. The Lorenz System is a simple model with three variables and a few parameters but exhibits very chaotic dynamics. It is therefore a good model to test the approach with. The Lorenz model is defined as follows:</p><script type="math/tex">\begin{align*}
\frac{dx}{dt} &= \sigma (y - x) \\
\frac{dy}{dt} &= x (\rho - z) - y \\
\frac{dz}{dt} &= x y - \beta z
\end{align*}
</script><p>The Lorenz system has 3 parameters (<script type="math/tex">\sigma</script>, <script type="math/tex">\beta</script>, and <script type="math/tex">\rho</script>), 3 variables (<script type="math/tex">x</script>, [<code>y</code>]($mathtex, and <script type="math/tex">z</script>. I use a set of summary statistics (mean, covariance, correlation, eigvalues, lyapunov exponents) which are generated by running the model with a bunch of times with a lot of different parameters and initial conditions (e.g. 10000 simulations). These I fed into a density estimator together with the parameters. The neural network then learned the relation between the summary statistics and the parameters.</p><p>Hopefully the application becomes clear when we turn the problem on itâ€™s head and perform inference. For example, I generate some data by running a single simulation of the model with parameters <script type="math/tex">\sigma = 10</script>, <script type="math/tex">\beta = \frac{8}{3}</script>, and <script type="math/tex">\rho = 28</script>. Then this data is given to the neural network whose parameters now generate distributions from which I can take a number of samples (e.g. 1000 samples). If I then plot these density of the samples, which are now in the parameter space you can easily see that the neural network returns values very close to the ground truth.</p><p><figure><img src="/blog/sbi_lorenz/sbi_lorenz_pairplot.png">
<figcaption><em>Figure 2: Pairplot from inference procedure of the Lorenz model.</em></figcaption></figure> This is a toy example, but not a trivial one. The Lorenz system has chaotic dynamics which are common in a lot of systems found in nature. The ability to infer parameters from systems from which we can only measure chaotic and unpredictable observations is very interesting. In a follow-up post I will dive into how the same technique can be applied to infer neural activations from brain activity measurements.</p><h3>Notes</h3><ul><li>The search space grows very fast with the number of parameters that are of interest for the inference.</li><li>The quality of the inference depends heavily on the quality and diversity of the simulations used for training.</li><li>Choosing appropriate summary statistics is crucial for the performance of the neural density estimator.</li><li>It is important to validate the trained model with independent test data to ensure its generalizability.</li><li>Hyperparameter tuning of the neural network can significantly impact the accuracy of the inferred parameters.</li><li>Visualization of the inferred parameter distributions can provide insights into the uncertainty and (marginal) correlations between parameters.</li></ul><p>The code for this little experiment is publicly available on <a href="https://github.com/krisevers/lorenz_sbi" target="_blank">GitHub</a>.</p><h3>References</h3><ol><li>Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., & Lakshminarayanan, B. (2019). <em>Normalizing Flows for Probabilistic Modeling and Inference</em>. <em>Journal of Machine Learning Research</em>, 22(57), 1-64.</li><li>Cranmer, K., Brehmer, J., & Louppe, G. (2020). <em>The frontier of simulation-based inference</em>. <em>Proceedings of the National Academy of Sciences</em>, 117(48), 30055-30062.</li><li>Lorenz, E. N. (1963). <em>Deterministic nonperiodic flow</em>. <em>Journal of the Atmospheric Sciences</em>, 20(2), 130-141.</li></ol></div>
  <div>
     <!--<h2>Post list</h2>-->
    <div></div>
  </div>

    <footer>
      <hr>
      <h2>find me here:</h2>
      <a class="link" href="krisevers14@gmail.com">email: krisevers14@gmail.com</a>
      <a class="link" href="https://github.com/krisevers">github: github.com/krisevers</a>
      <a class="link" href="https://orcid.org/0000-0002-3386-1259">publications : https://orcid.org/0000-0002-3386-1259</a>
    </footer>

  </body>
</html>
